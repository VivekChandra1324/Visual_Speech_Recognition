# -*- coding: utf-8 -*-
"""Deep_learning_Project_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WYB4bwovietpGfSTE2Cv0fNW6H2dDZCu
"""

#!pip install opencv-python matplotlib imageio gdown tensorflow
import os
import cv2
import dlib
import numpy as np
import tensorflow as tf
from matplotlib import pyplot as plt
from typing import List
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv3D, Activation, MaxPool3D, TimeDistributed, Flatten, Bidirectional, LSTM, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler
import gdown
import imageio

file_id = '1shZ5PTNJdKv3Ms_QHSz-hHKGqkfPw6Np'
url = f'https://drive.google.com/uc?id={file_id}'
output = 'data.zip'
gdown.download(url, output, quiet=False)
gdown.extractall('data.zip')

"""## Organizing video files and corresponding alignments"""

def organize_data(videos_dir, alignments_dir):
    video_files = []
    alignment_files = []

    # Iterate over files in the videos directory
    for file in os.listdir(videos_dir):
        video_path = os.path.join(videos_dir, file)

        # Check if the file is a video file
        if file.endswith(".mpg"):
            video_files.append(video_path)

            # Find the corresponding alignment file
            alignment_file = file.replace(".mpg", ".align")
            alignment_path = os.path.join(alignments_dir, alignment_file)

            if os.path.exists(alignment_path):
                alignment_files.append(alignment_path)
            else:
                print(f"Alignment file not found for video: {file}")

    # Sort the video and alignment files
    video_files.sort()
    alignment_files.sort()

    # Create a list of tuples (video_file, alignment_file)
    data_pairs = list(zip(video_files, alignment_files))

    return data_pairs

videos_dir = './data/s1'
alignments_dir = './data/alignments/s1'

data_pairs = organize_data(videos_dir, alignments_dir)

# Print the organized data pairs
for video_file, alignment_file in data_pairs:
    print(f"Video: {video_file}")
    print(f"Alignment: {alignment_file}")
    print()

file_id = '1sJ-ujHfz9mykulBOa9cu5jZDQrfEOh4y'
url = f'https://drive.google.com/uc?id={file_id}'
output = 'shape_predictor_68_face_landmarks.dat'

# Download the file
gdown.download(url, output, quiet=False)

"""## Processing lip regions from the videos"""

def load_video(path: str, predictor_path: str) -> List[float]:
    # Load the shape predictor model
    predictor = dlib.shape_predictor(predictor_path)

    # Load the dlib face detector
    detector = dlib.get_frontal_face_detector()

    cap = cv2.VideoCapture(path)
    frames = []
    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):
        ret, frame = cap.read()
        if not ret:
            break

        # Convert the frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Detect faces in the frame
        faces = detector(gray)

        # Process each detected face
        for face in faces:
            # Get the facial landmarks
            landmarks = predictor(gray, face)

            # Extract the lip region coordinates
            lip_points = []
            for i in range(48, 68):
                x = landmarks.part(i).x
                y = landmarks.part(i).y
                lip_points.append((x, y))

            # Convert the lip points to a NumPy array
            lip_points = np.array(lip_points)

            # Calculate the bounding box of the lip region
            x_min, y_min = np.min(lip_points, axis=0)
            x_max, y_max = np.max(lip_points, axis=0)

            # Extract the lip region from the frame
            lip_region = frame[y_min:y_max, x_min:x_max]

            # Convert the lip region to grayscale
            lip_region = cv2.cvtColor(lip_region, cv2.COLOR_BGR2GRAY)

            # Resize the lip region to a fixed size
            lip_region = cv2.resize(lip_region, (64, 64))

            frames.append(lip_region)

    cap.release()

    # Convert frames to float32 and normalize
    frames = tf.cast(frames, tf.float32) / 255.0

    return frames

"""## Sample Visualization of the frame"""

def visualize_lip_region(frame, predictor_path):
    # Load the shape predictor model
    predictor = dlib.shape_predictor(predictor_path)

    # Load the dlib face detector
    detector = dlib.get_frontal_face_detector()

    # Convert the frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect faces in the frame
    faces = detector(gray)

    # Process each detected face
    for face in faces:
        # Get the facial landmarks
        landmarks = predictor(gray, face)

        # Extract the lip region coordinates
        lip_points = []
        for i in range(48, 68):
            x = landmarks.part(i).x
            y = landmarks.part(i).y
            lip_points.append((x, y))

        # Convert the lip points to a NumPy array
        lip_points = np.array(lip_points)

        # Create a mask for the lip region
        mask = np.zeros_like(gray)
        cv2.fillPoly(mask, [lip_points], 255)

        # Create a colored lip region
        lip_region_color = cv2.bitwise_and(frame, frame, mask=mask)

        # Create a grayscale frame with the colored lip region
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2BGR)
        result = cv2.addWeighted(gray_frame, 1, lip_region_color, 1, 0)

    return result

# Specify the path to the shape predictor model
predictor_path = './shape_predictor_68_face_landmarks.dat'

# Load a sample video frame
video_path = './data/s1/bbaf2n.mpg'
cap = cv2.VideoCapture(video_path)
ret, frame = cap.read()
cap.release()

# Visualize the lip region on the frame
frame_with_lip_region = visualize_lip_region(frame, predictor_path)

# Display the frame with the lip region highlighted
plt.figure(figsize=(8, 8))
plt.imshow(cv2.cvtColor(frame_with_lip_region, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()

"""## character-to-number and number-to-character mapping"""

vocab = [x for x in "abcdefghijklmnopqrstuvwxyz'?!123456789 "]

char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token="")
num_to_char = tf.keras.layers.StringLookup(
    vocabulary=char_to_num.get_vocabulary(), oov_token="", invert=True
)

print(
    f"The vocabulary is: {char_to_num.get_vocabulary()} "
    f"(size ={char_to_num.vocabulary_size()})"
)

char_to_num.get_vocabulary()

char_to_num(['v','i','v','e','k'])

num_to_char([22,9,22,5,11])

def load_alignments(path:str) -> List[str]:
    with open(path, 'r') as f:
        lines = f.readlines()
    tokens = []
    for line in lines:
        line = line.split()
        if line[2] != 'sil':
            tokens = [*tokens,' ',line[2]]
    return char_to_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]

def create_dataset(data_pairs, predictor_path):
    videos = []
    alignments = []

    for video_file, alignment_file in data_pairs:
        video_frames = load_video(video_file, predictor_path)
        alignment_tokens = load_alignments(alignment_file)

        videos.append(video_frames)
        alignments.append(alignment_tokens)

    # Pad the video frames and alignments to the maximum length
    max_video_length = max(len(video) for video in videos)
    max_alignment_length = max(len(alignment) for alignment in alignments)

    padded_videos = []
    padded_alignments = []

    for video, alignment in zip(videos, alignments):
        # Pad video frames
        padding_frames = max_video_length - len(video)
        padded_video = tf.pad(video, [[0, padding_frames], [0, 0], [0, 0]])
        padded_videos.append(padded_video)

        # Pad alignments
        padding_tokens = max_alignment_length - len(alignment)
        padded_alignment = tf.pad(alignment, [[0, padding_tokens]])
        padded_alignments.append(padded_alignment)

    # Convert the padded videos and alignments to TensorFlow datasets
    video_dataset = tf.data.Dataset.from_tensor_slices(padded_videos)
    alignment_dataset = tf.data.Dataset.from_tensor_slices(padded_alignments)

    # Zip the video and alignment datasets
    dataset = tf.data.Dataset.zip((video_dataset, alignment_dataset))

    return dataset

# Specify the path to the shape predictor model
predictor_path = './shape_predictor_68_face_landmarks.dat'

# Create the dataset
dataset = create_dataset(data_pairs, predictor_path)

# Get the total number of samples in the dataset
total_samples = len(data_pairs)

# Specify the split ratio
train_ratio = 0.8
train_samples = int(total_samples * train_ratio)

# Split the dataset into training and testing sets
train_dataset = dataset.take(train_samples)
test_dataset = dataset.skip(train_samples)

# Reshape the video data to match the model architecture
train_dataset = train_dataset.map(lambda video, alignment: (tf.expand_dims(video, axis=-1), alignment))
test_dataset = test_dataset.map(lambda video, alignment: (tf.expand_dims(video, axis=-1), alignment))


# Define the maximum label length
max_label_length = 31
train_dataset = train_dataset.map(lambda video, alignment: (tf.expand_dims(video, axis=0), tf.reshape(alignment, (-1, max_label_length))))
test_dataset = test_dataset.map(lambda video, alignment: (tf.expand_dims(video, axis=0), tf.reshape(alignment, (-1, max_label_length))))



# Print the dataset information
print("Training Dataset:")
for video, alignment in train_dataset.take(1):
    print("Video shape:", video.shape)
    print("Alignment shape:", alignment.shape)

print("Testing Dataset:")
for video, alignment in test_dataset.take(1):
    print("Video shape:", video.shape)
    print("Alignment shape:", alignment.shape)

def CTCLoss(y_true, y_pred):
    batch_len = tf.cast(tf.shape(y_true)[0], dtype="int64")
    input_length = tf.cast(tf.shape(y_pred)[1], dtype="int64")
    label_length = tf.cast(tf.shape(y_true)[1], dtype="int64")

    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype="int64")
    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype="int64")

    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)
    return loss


def scheduler(epoch, lr):
    if epoch < 30:
        return lr
    else:
        return lr * tf.math.exp(-0.1)

"""## Model Architecture"""

'''

# Define the model architecture
model = Sequential()
model.add(Conv3D(128, 3, input_shape=(75, 64, 64, 1), padding='same'))

model.add(Activation('relu'))
model.add(MaxPool3D((1, 2, 2)))

model.add(Conv3D(256, 3, padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1, 2, 2)))

model.add(Conv3D(75, 3, padding='same'))
model.add(Activation('relu'))
model.add(MaxPool3D((1, 2, 2)))

model.add(TimeDistributed(Flatten()))

model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))
model.add(Dropout(.5))

model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))
model.add(Dropout(.5))

model.add(Dense(char_to_num.vocabulary_size() + 1, kernel_initializer='he_normal', activation='softmax'))

model.summary()

'''

"""## First Trained model(20 epochs)"""

'''

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss)

# Define callbacks
checkpoint_callback = ModelCheckpoint(os.path.join('models','checkpoint'), monitor='loss', save_weights_only=True)
schedule_callback = LearningRateScheduler(scheduler)

# Train the model
model.fit(train_dataset, epochs=20, callbacks=[checkpoint_callback, schedule_callback])

# Evaluate the model
model.evaluate(test_dataset)

import os
from google.colab import files

# Save the model
model_name = "lip_reading_model.h5"
model.save(model_name)

'''

"""## using the saved model for retraining (40 epochs)"""

'''

from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler

# Load the saved model
model = load_model('lip_reading_model_20.h5', custom_objects={'CTCLoss': CTCLoss})

# Define callbacks
checkpoint_callback = ModelCheckpoint(os.path.join('models','checkpoint'), monitor='loss', save_weights_only=True)
schedule_callback = LearningRateScheduler(scheduler)

# Continue training the model for an additional 20 epochs
model.fit(train_dataset, epochs=20, callbacks=[checkpoint_callback, schedule_callback])

# Evaluate the model on the test dataset
model.evaluate(test_dataset)


'''

"""## using the saved model for retraining (60 epochs)"""

# Load the saved model
model = load_model('lip_reading_model_40.h5', custom_objects={'CTCLoss': CTCLoss})

# Define callbacks
checkpoint_callback = ModelCheckpoint(os.path.join('models','checkpoint'), monitor='loss', save_weights_only=True)
schedule_callback = LearningRateScheduler(scheduler)

# Continue training the model for an additional 20 epochs
model.fit(train_dataset, epochs=20, callbacks=[checkpoint_callback, schedule_callback])

# Evaluate the model on the test dataset
model.evaluate(test_dataset)

import os
from google.colab import files


# Save the updated model
model_name = "lip_reading_model_60.h5"
model.save(model_name)

# Download the model
files.download(model_name)

# Function to decode the predictions
def decode_predictions(predictions):
    input_len = np.ones(predictions.shape[0]) * predictions.shape[1]
    results = tf.keras.backend.ctc_decode(predictions, input_length=input_len, greedy=True)[0][0]
    output_text = []
    for result in results:
        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode("utf-8")
        output_text.append(result)
    return output_text

# Make predictions on the test dataset
predictions = model.predict(test_dataset)
predicted_texts = decode_predictions(predictions)

# Print the original and predicted texts
for (video, alignment), predicted_text in zip(test_dataset, predicted_texts):
    original_text = tf.strings.reduce_join(num_to_char(alignment)).numpy().decode("utf-8")
    print(f"Original Text: {original_text}")
    print(f"Predicted Text: {predicted_text}")
    print()

def calculate_accuracy(original_texts, predicted_texts):
    total_characters = 0
    correct_characters = 0

    for original_text, predicted_text in zip(original_texts, predicted_texts):
        total_characters += len(original_text)
        correct_characters += sum(1 for a, b in zip(original_text, predicted_text) if a == b)

    accuracy = correct_characters / total_characters
    return accuracy


# Get the original texts
original_texts = []
for _, alignment in test_dataset:
    original_text = tf.strings.reduce_join(num_to_char(alignment)).numpy().decode("utf-8")
    original_texts.append(original_text)

# Calculate the accuracy
accuracy = calculate_accuracy(original_texts, predicted_texts)
print(f"Accuracy: {accuracy:.4f}")

"""## Predictions"""

# Load the shape predictor and the trained model
predictor_path = './shape_predictor_68_face_landmarks.dat'
model_path = 'lip_reading_model_60.h5'
predictor = dlib.shape_predictor(predictor_path)
detector = dlib.get_frontal_face_detector()
model = load_model(model_path, custom_objects={'CTCLoss': CTCLoss})

def load_video_and_predict(path: str, frame_count=75):
    cap = cv2.VideoCapture(path)
    frames = []
    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):
        ret, frame = cap.read()
        if not ret:
            break
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = detector(gray)
        for face in faces:
            landmarks = predictor(gray, face)
            lip_points = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(48, 68)])
            x_min, y_min = np.min(lip_points, axis=0)
            x_max, y_max = np.max(lip_points, axis=0)
            lip_region = frame[y_min:y_max, x_min:x_max]
            lip_region = cv2.cvtColor(lip_region, cv2.COLOR_BGR2GRAY)
            lip_region = cv2.resize(lip_region, (64, 64))
            frames.append(lip_region)
    cap.release()

    # Adjust the number of frames to match the model's expected input
    frames = frames[:frame_count]
    while len(frames) < frame_count:
        frames.append(np.zeros((64, 64), dtype=np.float32))

    frames = np.array(frames).astype('float32') / 255.0
    frames = np.expand_dims(frames, axis=-1)
    frames = np.expand_dims(frames, axis=0)
    return frames

def decode_predictions(predictions):
    input_len = np.ones(predictions.shape[0]) * predictions.shape[1]
    results = tf.keras.backend.ctc_decode(predictions, input_length=input_len, greedy=True)[0][0]
    output_text = []
    for result in results:
        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode("utf-8")
        output_text.append(result)
    return output_text

video_path = './lbij7s.mpg'
video_frames = load_video_and_predict(video_path)
predictions = model.predict(video_frames)

predicted_text = decode_predictions(predictions)
print("Predicted Text:", predicted_text)

video_path = './lbwk9s.mpg'
video_frames = load_video_and_predict(video_path)
predictions = model.predict(video_frames)

predicted_text = decode_predictions(predictions)
print("Predicted Text:", predicted_text)
